{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urban_dictionary_scraper\n",
    "import torch\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "import wiki_article \n",
    "import dictionary_definition\n",
    "import glob\n",
    "import modeling\n",
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dataclasses import dataclass\n",
    "from io import StringIO\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from scipy import stats\n",
    "import hashlib\n",
    "from collections import OrderedDict\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoints(base_dir):\n",
    "    checkpoint_dirs = glob.glob(f\"{base_dir}/checkpoint*\")\n",
    "    checkpoint_dirs.sort(key=lambda x: int(x[(x.index(\"checkpoint-\") + len(\"checkpoint-\")):]))\n",
    "    return checkpoint_dirs\n",
    "modeling_gpt\n",
    "def evaluate_lm_checkpoints(base_dir, validation_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    for d in get_checkpoints(base_dir):\n",
    "        model = AutoModelWithLMHead.from_pretrained(d).to('cuda')\n",
    "        refined_model_eval = wiki_article.lm_eval(model, tokenizer, validation_path)\n",
    "        print(f\"{d}: {refined_model_eval}\")\n",
    "tokenizer\n",
    "def evaluate_title_checkpoints(base_dir, validation_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")print(parsed_urban_dictionary_scraperpage.body.prettify())\n",
    "    for d in get_checkpoints(base_dir):\n",
    "        model = AutoModelWithLMHead.from_pretrained(d).to('cuda')\n",
    "        refined_model_eval = wiki_article.run_title_evaluation(model, tokenizer, validation_path)\n",
    "        print(f\"{d}: m={refined_model_eval.mean}, v={refined_model_eval.variance}\")\n",
    "\n",
    "# evaluate_lm_checkAutoModelWithLMHead, AutoTokenizer, points(\"models/wikitext_103_stride_512_v0/\", \"data/wikitext-103-title-train/wiki_title.valid.raw\")\n",
    "#print(glob.glob(\"models/wikitext_103_stride_512_v0/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/en_dictionary_parsed_randomized.pickle\", \"rb\") as f:\n",
    "    parsed_dictionary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_blacklist = set()\n",
    "for word in parsed_dictionary:\n",
    "    potential_blacklist.add(word.word)\n",
    "    potential_blacklist.update(word.derivatives)\n",
    "print(len(parsed_dictionary))\n",
    "print(len(potential_blacklist))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens(datasets.SpecialTokens.special_tokens_dict())\n",
    "args = SimpleNamespace()\n",
    "args.block_size = 768\n",
    "dataset = datasets.ParsedDictionaryDefinitionDataset(tokenizer, args, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_set = list(itertools.chain.from_iterable(dataset._make_examples(tokenizer, e) for e in parsed_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"{len(flattened_set)} from {len(parsed_dictionary)} entries\")\n",
    "word = tokenizer.encode(\"vitellogenin\")\n",
    "print(tokenizer.decode(dataset.bos_token_ids  + [1] + dataset.eos_token_ids))\n",
    "print(tokenizer.decode(tokenizer.encode(\"<|bod|>\\\"<|eod|>\")))\n",
    "\n",
    "print(f\"\\\"{tokenizer.decode(dataset.pos_sep_ids)}\\\"\")\n",
    "tokenizer.decode(dataset._make_examples(tokenizer, parsed_dictionary[0])[0])\n",
    "# for example in random.choices(flattened_set, k=20):\n",
    "#     print(tokenizer.decode(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in dataset._make_examples(tokenizer, parsed_dictionary[10430]):\n",
    "    print(tokenizer.decode(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/all_words.pickle\", \"rb\") as f:\n",
    "    #words = pickle.load(f)\n",
    "    #items = list(words.items())\n",
    "    random.shuffle(items)\n",
    "    items = OrderedDict(items)\n",
    "    \n",
    "with open(\"data/all_words_randomized.pickle\", \"wb\") as f:\n",
    "    pickle.dump(items, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_dictionary_scraper.UrbanDictionaryDataset._make_examples(tokenizer, words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrefined_model_eval = wiki_article.run_title_evaluation(urban_dictionary_scrapermodel, tokenizer, \"wikitext-103-raw/wiki.valid.raw\")\n",
    "unrefined_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"output_103/\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_model_eval = wiki_article.run_title_evaluation(model, tokenizer, \"wikitext-103-raw/wiki.valid.raw\")\n",
    "refined_model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = f\"\\\"TITLE\\\" is a song collaboration by Chinese artist Pamela Chen and Canadian singer Thomas Dimson, first released independently in March 2020. After gaining popularity amongst the cat community, the single was re-released by major label Columbia Records in May 2020. Pamela describes the song as being originally inspired by her two kittens, Apollo and Bean who once said meow.<bot>\"\n",
    "\n",
    "model =  modeling.GPT2LMHeadWithWeightedLossModel.from_pretrained(\"models/wikitext-103-raw-title-scale-20-lr5e-5\").to(\"cuda\")\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\").to('cuda')\n",
    "generated = model.generate(input, max_length=100, num_return_sequences=100, temperature=1)\n",
    "\n",
    "print(f\"Prompt text: {sequence}\")\n",
    "for i in range(generated.size()[0]):\n",
    "    sentence_tokens = generated[i, :].tolist()\n",
    "    decoded = tokenizer.decode(sentence_tokens)\n",
    "    m = re.search(r\"<bot>(.*?)<eot>\", decoded)\n",
    "    if m:urban_dictionary_scraper\n",
    "        print(f\"{i}) {m.groups(1)}\")\n",
    "    else:\n",
    "        print(f\"{i}) Didn't work\")\n",
    "    \n",
    "\n",
    "resulting_string = tokenizer.decode(generated.tolist()[0])\n",
    "# print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in entries: \n",
    "    m = re.match(r\"\\s*\" + re.escape(entry.title) + r\"\\d*\\s*(\\|[^|]*\\|)?\\s*\", entry.entry_str)\n",
    "    if m:\n",
    "        trainable_entry = entry.entry_str[m.span()[1]:].strip()\n",
    "        if not trainable_entry:\n",
    "            raise RuntimeError(f\"Bad entry for {entry.title}: '{entry.entry_str}'\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Couldn't match {entry.title} on '{entry.entry_str}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = \"data/com_apple_MobileAsset_DictionaryServices_dictionaryOSX/69b7ab1cf0f75ad16bf6662b0a77fbfd36b7941f.asset/AssetData/New Oxford American Dictionary.dictionary/Contents/Resources/Body.data\"\n",
    "with open(dictionary_path, \"rb\") as f:\n",
    "    valid_words = {e.title.upper() for e in dictionary_definition.DictionaryDefinition.gen_from_apple_dictionary(f)}full_dataset = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  modeling.GPT2LMHeadWithWeightedLossModel.from_pretrained(\"models/dictionary-scale-10-lr5e-5\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dictionary_definition.generate_words(\n",
    "    tokenizer, model, allow_proper_nouns=False, blacklist=valid_words, num=1000, max_iterations=40\n",
    ")\n",
    "words.sort(key=lambda x: x.title)\n",
    "for w in words:\n",
    "    print(f\"{w} {w.entry_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words.tsv\", \"w\") as f:\n",
    "    for word in words:\n",
    "        f.write(f\"{word.title}\\t{word.entry_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens(datasets.SpecialTokens.special_tokens_dict())\n",
    "blacklist = set((x.lower() for x in itertools.chain.from_iterable(\n",
    "    [e.word] + e.derivatives\n",
    "    for e in pickle.load(open(f\"data/en_dictionary_parsed_randomized.pickle\", \"rb\")))\n",
    "))\n",
    "model = AutoModelWithLMHead.from_pretrained(\"models/en_dictionary_parsed_lr_00001/checkpoint-120000\").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_words(words, f):\n",
    "    for word in words:\n",
    "        word_str = [word.word]\n",
    "        if word.pos:\n",
    "            word_str.append(f\"/{word.pos}/\")\n",
    "        if word.topic:\n",
    "            word_str.append(f\"[{word.topic}]\")\n",
    "        print(\" \".join(word_str), file=f)\n",
    "        print(f\"\\t{word.definition}\", file=f)\n",
    "        print(f\"\\t\\\"{word.example}\\\"{' |e|' if word.from_example_expansion else ''}\", file=f))\n",
    "\n",
    "        print(\"\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.sort(key=lambda x: x.word)\n",
    "with open(\"words_with_examples.txt\", \"w\") as f:\n",
    "    print_words(words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, stats = datasets.ParsedDictionaryDefinitionDataset.generate_words(\n",
    "    tokenizer, model,\n",
    "    num=500,\n",
    "    max_iterations=40, \n",
    "    blacklist=blacklist, \n",
    "    do_example_expansion=True, \n",
    "    generation_args=dict(\n",
    "        top_k=300,\n",
    "        num_return_sequences=100,\n",
    "        max_length=512,\n",
    "        do_sample=True,\n",
    "    ),\n",
    "    expansion_generation_overrides=dict(\n",
    "        top_k=50,\n",
    "        num_return_sequences=10,\n",
    "        do_sample=True,\n",
    "    ),\n",
    "    num_expansion_candidates=10,\n",
    "    filter_proper_nouns=True,\n",
    ")\n",
    "\n",
    "print(stats)\n",
    "print()\n",
    "print_words(words, sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from datasets import SpecialTokens\n",
    "\"\"\"\n",
    "input_str = f\"{tokenizer.bos_token}\"\n",
    "input_str = \"<|bod|>corner<|pos|>noun<|bd|>a point or space in a hierarchy that is within the order to which it moves along the axis.<|eod|>\"\n",
    "input = tokenizer.encode(input_str, return_tensors=\"pt\").to(\"cuda\")\n",
    "max_length = 512\n",
    "\n",
    "generated = model.generate(\n",
    "    input_ids=input, \n",
    "    max_length=max_length, \n",
    "    num_return_sequences=5, \n",
    "    temperature=1.0,\n",
    "    top_k=1000,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_ids=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "break_specials = [\n",
    "    SpecialTokens.BOS_TOKEN, SpecialTokens.EOS_TOKEN, SpecialTokens.DEFINITION_SEP,\n",
    "    SpecialTokens.EXAMPLE_SEP, SpecialTokens.TOPIC_SEP, SpecialTokens.POS_SEP \n",
    "]\n",
    "break_special_ids = [tokenizer.encode(e, add_prefix_space=False)[0] for e in break_specials]\n",
    "break_special_token_map = {s: i for s, i in zip(break_specials, break_special_ids)}\n",
    "\n",
    "\n",
    "for i in range(generated.size()[0]):\n",
    "    sentence_tokens = generated[i, :].tolist()\n",
    "    \n",
    "    \n",
    "    accum = []\n",
    "    last_special = None\n",
    "    sep_map = {}\n",
    "    for token_id in sentence_tokens:\n",
    "        if token_id in break_special_ids:\n",
    "            if last_special is not None:\n",
    "                sep_map[last_special] = accum\n",
    "                accum = []\n",
    "                last_special = token_id\n",
    "            else:\n",
    "                last_special = token_id\n",
    "        else:\n",
    "            accum.append(token_id)\n",
    "            \n",
    "    sep_map[last_special] = accum\n",
    "    accum = []\n",
    "    \n",
    "    decode_sep_map = {\n",
    "        tokenizer.decode([k]): tokenizer.decode(v) for k, v in sep_map.items()\n",
    "    }\n",
    "        \n",
    "    print(decode_sep_map)\n",
    "    \n",
    "    # decoded = tokenizer.decode([e for e in sentence_tokens if e != tokenizer.pad_token_id])\n",
    "    print(decoded)  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"a bc\", add_prefix_space=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = set(e.title for e in pickle.load(open(\"data/all_words.pickle\", \"rb\")).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  modeling.GPT2LMHeadWithWeightedLossModel.from_pretrained(\n",
    "    \"models/urban_dictionary_cleaned_top_def_mu02_lr_0_000005_tw40\"\n",
    ").to(\"cuda\")\n",
    "tw40_words = urban_dictionary_scraper.generate_words(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    blacklist=blacklist,\n",
    "    num=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tw1_words, open(\"data/labeling/tw1_words.pickle\", \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(tw40_words, open(\"data/labeling/tw40_words.pickle\", \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        (\n",
    "            word.word,\n",
    "            word.definition,\n",
    "            word.example.replace(,\n",
    "            \"tw1\" if i < len(tw1_words) else \"tw2\",\n",
    "        )\n",
    "        for i, word in enumerate(itertools.chain(\n",
    "            tw1_words,\n",
    "            tw40_words\n",
    "        ))\n",
    "    ],\n",
    "    columns=(\"word\", \"definition\", \"example\", \"dataset\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no_dataset = sample[:]\n",
    "sample_no_dataset.to_csv(\"fun.csv\", index=False, columns=[\"word\", \"definition\", \"example\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens(datasets.SpecialTokens.special_tokens_dict())\n",
    "model =  AutoModelWithLMHead.from_pretrained(\"models/en_dictionary_parsed_lr_00005/checkpoint-50000\").to(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
